MultiLog模型实现需求文档
1. 项目目标
基于KDD '24会议论文《Multivariate Log-based Anomaly Detection for Distributed Database》，使用Python及主流深度学习框架（如PyTorch）实现MultiLog模型。 


此实现的核心目标是复现论文中描述的模型架构，使其能够处理预先准备好的、来自多节点的日志数据集，并对整个集群的状况（正常/异常）进行分类。

2. 模型架构总览
MultiLog是一个两阶段的监督式学习框架，专为分布式数据库的多元日志异常检测设计。 




阶段一：独立评估 (Standalone Estimation)

此模块在每个数据库节点上独立运行。 
它负责处理单个节点的日志，从中提取序列、量化、语义三种维度的信息，并通过增强模型生成一个代表异常可能性的概率列表。 


阶段二：集群分类器 (Cluster Classifier)

此模块聚合来自所有节点的概率列表。 


它首先将长度不一的概率列表标准化，然后通过一个元分类器对整个集群的状态做出最终的“正常”或“异常”判断。 

图示：MultiLog框架图，对应原文Figure 6 

3. 阶段一：Standalone Estimation模块实现细节
此模块是整个流程的基础，负责从原始日志中提取高质量的特征。它包含以下几个串行步骤：

3.1. 日志解析与分组
输入: 单个节点的原始日志文本文件。
步骤:
日志解析: 使用 Drain3 算法将非结构化的原始日志文本解析为结构化的日志事件（Log Event）。 

日志分组: 采用固定时间窗口（fixed time window）的方法，将解析后的日志事件序列切分为多个固定长度的日志组 S_j。 

3.2. 日志嵌入 (三路并行处理)
对于每一个日志组 S_j，需要并行提取以下三种特征嵌入：

a. 序列嵌入 (Sequential Embedding)
描述: 直接使用日志组中的事件ID序列 E_j = (e(s_1), e(s_2), ..., e(s_M)) 作为输入。  这代表了日志发生的时间顺序模式。 



b. 量化嵌入 (Quantitative Embedding)
描述: 统计每个日志组 S_j 中，不同类型的日志事件 e_k 出现的频率。 

输出: 一个计数向量 C_j = (c_j(e_1), c_j(e_2), ..., c_j(e_n))。 

c. 语义嵌入 (Semantic Embedding)
描述: 提取每个日志事件的文本语义信息。 
步骤:
预处理: 对每个日志事件的文本，移除无实际意义的非字符token和停用词，并使用驼峰命名法（Camel Case）拆分复合词。 
词向量化: 使用预训练的 FastText 模型（基于Common Crawl Corpus，300维）将每个单词转换为词向量 v。 
事件向量化: 使用 TF-IDF 计算每个单词在事件中的权重 ε。  通过加权求和的方式，将事件中所有单词的向量聚合成一个最终的事件语义向量 V。 


输出: 一个由事件语义向量组成的序列 V_j = (v(e(s_1)), v(e(s_2)), ..., v(e(s_M)))。 
3.3. 信息增强 (LSTM + Self-Attention)
将上述三种嵌入分别输入并行的增强模块中。

模型: 对每一路嵌入（E, C, V），都使用一个 LSTM 模型进行处理。 
输入: E, C, V 三个嵌入序列。
处理:
LSTM 处理输入序列，得到所有时间步的隐藏状态 H = [h_1, h_2, ..., h_M]。 
在隐藏状态序列 H 上应用 Self-Attention 机制，计算出一个加权的上下文向量 c。 
注意力打分函数为: score(h_m, h_M) = h_m^T * W * h_M。 
将上下文向量 c 和LSTM的最后一个隐藏状态 h_M 拼接起来，得到该路特征的增强输出 EC = [c; h_M]。 
3.4. 模块输出
将三路增强后的输出向量 EC_E, EC_C, EC_V 拼接成一个大向量。 
将这个大向量送入一个全连接层（FC），输出一个概率值 p，表示该日志组为异常的概率。 
最终输出: 对于节点 Node_i，此模块的最终输出是一个长度可变的概率列表 P_i = [p_1, p_2, ..., p_k_i]。 

4. 阶段二：Cluster Classifier模块实现细节
此模块负责汇总所有节点的信息，做出最终决策。

4.1. 概率标准化 (AutoEncoder)
目的: 解决各节点输出的概率列表 P_i 长度不同的问题。 
输入: 来自N个节点的概率列表集合 {P_1, P_2, ..., P_N}。
步骤:
编码器 (Encoder):
将每个输入的概率列表 P_i 通过填充或截断的方式处理成一个固定长度 β 的向量。 
使用一个编码器 f_enc（由三个带ReLU激活函数的线性层构成）将其映射到一个固定大小为 μ 的隐向量 Z_i。 
解码器 (Decoder):
使用一个解码器 f_dec 尝试从隐向量 Z_i 中重建原始的概率列表。 
训练: 使用均方误差损失（MSELoss）来训练这个AutoEncoder，目的是让重建误差最小化。 
输出: N个标准化的、固定长度的隐向量 {Z_1, Z_2, ..., Z_N}。 
4.2. 元分类 (Meta-Classification)
目的: 根据所有节点的标准化信息，对整个集群进行分类。
输入: 隐向量集合 {Z_1, Z_2, ..., Z_N}。
步骤:
将所有隐向量 Z_i 拼接成一个单一的、更长的向量 Z。 

将向量 Z 输入到一个元分类器 f_meta 中。 
该分类器是一个包含一个隐藏层和一个Softmax输出层的神经网络。 
输出: 最终的集群状态预测结果：“正常 (Normal)” 或 “异常 (Anomalous)”。 
5. 关键参数与实现细节
日志解析器: Drain3 

时间窗口大小: 5秒 
词向量: FastText (300维) 
AutoEncoder输入长度 (β): 128 
AutoEncoder隐向量大小 (μ): 32 